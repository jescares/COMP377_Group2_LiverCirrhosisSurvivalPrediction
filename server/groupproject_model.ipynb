{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>N_Days</th>\n",
       "      <th>Status</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Alk_Phos</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>Tryglicerides</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Prothrombin</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>D</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>21464</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>14.5</td>\n",
       "      <td>261.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1718.0</td>\n",
       "      <td>137.95</td>\n",
       "      <td>172.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4500</td>\n",
       "      <td>C</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>20617</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1.1</td>\n",
       "      <td>302.0</td>\n",
       "      <td>4.14</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7394.8</td>\n",
       "      <td>113.52</td>\n",
       "      <td>88.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1012</td>\n",
       "      <td>D</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>25594</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>S</td>\n",
       "      <td>1.4</td>\n",
       "      <td>176.0</td>\n",
       "      <td>3.48</td>\n",
       "      <td>210.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>96.10</td>\n",
       "      <td>55.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1925</td>\n",
       "      <td>D</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>19994</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>S</td>\n",
       "      <td>1.8</td>\n",
       "      <td>244.0</td>\n",
       "      <td>2.54</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6121.8</td>\n",
       "      <td>60.63</td>\n",
       "      <td>92.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1504</td>\n",
       "      <td>CL</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>13918</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3.4</td>\n",
       "      <td>279.0</td>\n",
       "      <td>3.53</td>\n",
       "      <td>143.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>113.15</td>\n",
       "      <td>72.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  N_Days Status             Drug    Age Sex Ascites Hepatomegaly Spiders  \\\n",
       "0   1     400      D  D-penicillamine  21464   F       Y            Y       Y   \n",
       "1   2    4500      C  D-penicillamine  20617   F       N            Y       Y   \n",
       "2   3    1012      D  D-penicillamine  25594   M       N            N       N   \n",
       "3   4    1925      D  D-penicillamine  19994   F       N            Y       Y   \n",
       "4   5    1504     CL          Placebo  13918   F       N            Y       Y   \n",
       "\n",
       "  Edema  Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  \\\n",
       "0     Y       14.5        261.0     2.60   156.0    1718.0  137.95   \n",
       "1     N        1.1        302.0     4.14    54.0    7394.8  113.52   \n",
       "2     S        1.4        176.0     3.48   210.0     516.0   96.10   \n",
       "3     S        1.8        244.0     2.54    64.0    6121.8   60.63   \n",
       "4     N        3.4        279.0     3.53   143.0     671.0  113.15   \n",
       "\n",
       "   Tryglicerides  Platelets  Prothrombin  Stage  \n",
       "0          172.0      190.0         12.2    4.0  \n",
       "1           88.0      221.0         10.6    3.0  \n",
       "2           55.0      151.0         12.0    4.0  \n",
       "3           92.0      183.0         10.3    4.0  \n",
       "4           72.0      136.0         10.9    3.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change file path to where the cirrhosis.csv file is located\n",
    "file_path = \"./cirrhosis.csv\"\n",
    "cirrhosis_data = pd.read_csv(file_path)\n",
    "\n",
    "cirrhosis_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Bilirubin', 'Copper', 'Prothrombin', 'Stage', 'Edema_N',\n",
      "       'Hepatomegaly_N', 'Hepatomegaly_Y', 'Ascites_Y', 'Ascites_N',\n",
      "       'Alk_Phos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "cirrhosis_data.drop(['ID', 'N_Days'], axis=1, inplace=True)\n",
    "categorical_cols = [col for col in cirrhosis_data.columns if cirrhosis_data[col].dtype == 'object']\n",
    "numerical_cols = [col for col in cirrhosis_data.columns if cirrhosis_data[col].dtype != 'object']\n",
    "# Define the columns to encode\n",
    "columns_to_encode = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
    "\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the columns to one-hot encoded features\n",
    "encoded_features = encoder.fit_transform(cirrhosis_data[columns_to_encode])\n",
    "\n",
    "# Convert the encoded features to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_features.toarray(), columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "\n",
    "# Concatenate the encoded features with the original dataset\n",
    "encoded_data = pd.concat([cirrhosis_data, encoded_df], axis=1)\n",
    "encoded_data.dropna(inplace=True)\n",
    "# Drop the original columns that were encoded\n",
    "encoded_data.drop(columns_to_encode, axis=1, inplace=True)\n",
    "encoded_data['Status'] = encoded_data['Status'].astype('category').cat.codes\n",
    "\n",
    "# Calculate correlation coefficients with the target variable\n",
    "correlations = encoded_data.corr()['Status'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Select top features with highest absolute correlation coefficients\n",
    "selected_features = correlations.index[1:11]  # Exclude 'Status' column\n",
    "print(selected_features)\n",
    "numerical_cols_in_selected = [col for col in numerical_cols if col in selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dahye\\AppData\\Local\\Temp\\ipykernel_8980\\3502191662.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numerical_cols_in_selected] = scaler.fit_transform(X[numerical_cols_in_selected])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the logistic regression model\n",
    "# Splitting the data into features (X) and target (y)\n",
    "X = encoded_data[selected_features]\n",
    "y = encoded_data['Status']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols_in_selected] = scaler.fit_transform(X[numerical_cols_in_selected])\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7227272727272728\n",
      "Testing accuracy: 0.8392857142857143\n",
      "Model accuracy: 0.8392857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        30\n",
      "           2       1.00      0.65      0.79        26\n",
      "\n",
      "    accuracy                           0.84        56\n",
      "   macro avg       0.88      0.83      0.83        56\n",
      "weighted avg       0.88      0.84      0.83        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, logistic_model.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, logistic_model.predict(X_test))\n",
    "model_accuracy = logistic_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    "print(\"Model accuracy:\", model_accuracy)\n",
    "\n",
    "# Testing the model\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Display the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming model is your trained logistic regression model\n",
    "joblib.dump(logistic_model, 'logistic_regression_model.pkl')\n",
    "\n",
    "# Save the scaler for later use \n",
    "joblib.dump(encoder, 'encoder.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
